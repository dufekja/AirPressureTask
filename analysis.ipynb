{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Air pump failure analysis\n",
    "\n",
    "Following statements are deduced from given task `README.md`.\n",
    "\n",
    "### Important observations\n",
    "- sharp start and end of pressure (normally stable)\n",
    "- slow start, end or pressure drops could mean pump failure\n",
    "- typical air pump failure due to pressure drop in the first half of the cycle\n",
    "- pressure is saved as a time series (ordered, same intervals)\n",
    "\n",
    "### Data structure \n",
    "- MachineId, MeasurementId are Id's of machine and cycle\n",
    "- Pressure (kPa)\n",
    "- PumpFailed, SlowStart, SlowEnd (bool features)\n",
    "\n",
    "### Task\n",
    "- develop predictive model from `PumpFailed` and measure performance\n",
    "- explain model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "\n",
    "INDEXES = ['MachineId', 'MeasurementId']\n",
    "TARGET = ['PumpFailed']\n",
    "\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_unique_values(df: pd.DataFrame):\n",
    "    \"\"\" Print unique values for each feature in given dataframe \"\"\"\n",
    "    print('Unique values:')\n",
    "    for feature in df.columns:\n",
    "        print(f\"{feature:15} : {len(df[feature].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels = pd.read_csv('data/labels.csv')\n",
    "\n",
    "display(df_labels.head())\n",
    "display(df_labels.info())\n",
    "display(df_labels.isna().sum())\n",
    "\n",
    "# nan values in rows with measurementid = -1\n",
    "# I could rename machineid with new numeric counter or just use category type\n",
    "# I could drop rows with NaN values due to quite large amount of data \n",
    "#   - with more time I could try to replace it with most frequent values, median from distribution estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_unique_values(df_labels)\n",
    "\n",
    "print()\n",
    "print('Biggest MeasurementId: ', df_labels['MeasurementId'].max())\n",
    "\n",
    "# 556 uniquq machines and 8836 unique measurement cycles\n",
    "# bool features has nan, false, true -> drop\n",
    "# biggest measurementid is 8834 which should fit into int16 type for memory saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# drop NaN and change types\n",
    "df_labels = df_labels.dropna().astype({\n",
    "    'MachineId' : 'category',\n",
    "    'MeasurementId' : 'int16',\n",
    "    'PumpFailed' : 'bool',\n",
    "    'SlowStart' : 'bool',\n",
    "    'SlowEnd' : 'bool'\n",
    "})\n",
    "\n",
    "display(df_labels.info())\n",
    "\n",
    "# check target value counts\n",
    "print(df_labels['PumpFailed'].value_counts())\n",
    "\n",
    "# data with target value is inbalanced and binary, so I could just use prediction from Bernoulli distribution to create baseline model\n",
    "# inbalanced dataset could create biased model for the PumpFailed = False, so to solve this I can:\n",
    "#   - choose appropriate metric for validation (accuracy wont do in this case)\n",
    "#   - create completely new dataset using bootstrapping (harder to compute, but could be implemented if more time)\n",
    "#   - drop rows with PumpFailed = False to balance row counts (not optimal due to considerable dataset size loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pump bool feature combinations (all: 8)\n",
    "\n",
    "df_labels[['PumpFailed', 'SlowStart', 'SlowEnd']].drop_duplicates()\n",
    "\n",
    "# data does not contain any pumps with states:\n",
    "# (true, true, true) -> pump failed with both slow start and end\n",
    "# (true, true, false) -> pump failed only with slow start\n",
    "\n",
    "# later I can plot pressure in time for each of these combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_parquet('data/data.parquet', engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_data.head())\n",
    "display(df_data.info())\n",
    "display(df_data.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_unique_values(df_data)\n",
    "\n",
    "print()\n",
    "print('Largest pressure (kPa): ', df_data['Pressure'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = df_data.dropna().astype({\n",
    "    'MachineId' : 'category',\n",
    "    'MeasurementId' : 'int16',\n",
    "    'Pressure' : 'float32',\n",
    "})\n",
    "\n",
    "display(df_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge datasets using MachineId and MeasurementId\n",
    "df_merged = pd.merge(df_data, df_labels, on=INDEXES)\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merged dataframe pivoting\n",
    "\n",
    "The dataframe is currently in merged state with ordered ressure measurements.\n",
    "What I'll try to do now, is to pivot that dataframe, so that measured pressure values are shown ordered by time, but each column will represent unique measurement.\n",
    "\n",
    "With that format I could manipulate easily with pressure measurements as time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select both IDs and pressure features and set \"categorical\" dataframe index (set_index does not change order of rows)\n",
    "df_measurements = df_merged.loc[:, ['MachineId', 'MeasurementId', 'Pressure']].set_index(INDEXES)\n",
    "df_measurements.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column time where value is incremented by one when pressure belongs to same indexes\n",
    "# example: MachineId = 0_0_0 and MeasurementId = 0 will have increasing time series until it finds new Machine or Measurement ID \n",
    "df_measurements['Time'] = df_measurements.groupby(level=INDEXES, observed=False).cumcount()\n",
    "df_measurements.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it is just pivoting the table and replacing all shorter measurement sequences NaN values with zero \n",
    "df_measurements = df_measurements.pivot_table(index=INDEXES, columns='Time', values='Pressure', aggfunc='first', fill_value=0)\n",
    "display(df_measurements.head())\n",
    "\n",
    "assert df_measurements.shape[0] == df_labels.shape[0] # row count sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now I'll just merge it with dataframe containing bool features and target value\n",
    "df_all = pd.merge(df_labels.set_index(INDEXES), df_measurements, left_index=True, right_index=True).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test correlation between target and binary features\n",
    "df_labels[['PumpFailed', 'SlowStart', 'SlowEnd']].corr().round(2)\n",
    "\n",
    "# from correlation table I can't say that data are correlated or not\n",
    "# note: correlation is not the best test for binary features and there should be also tested if data are from norm. distribution,\n",
    "# but it could still be good indicator for doing more testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beacause of only 6 bool feature combinations, I will plot example presure in time data for each of them \n",
    "\n",
    "# plot config\n",
    "xlim, ylim = (50, 350), (-0.1, 1.7)\n",
    "fig, axes = plt.subplots(3, 2, figsize = (14, 6))\n",
    "fig.tight_layout()\n",
    "\n",
    "# get possible combinations\n",
    "combinations = df_all[['PumpFailed', 'SlowStart', 'SlowEnd']].drop_duplicates()\n",
    "\n",
    "# loop for each bool feature combination\n",
    "for i, [_ , [fail, start, end]] in enumerate(combinations.iterrows()):\n",
    "    \n",
    "    # get first pump matching feature combination\n",
    "    first_pump_match = df_all[(df_all['PumpFailed'] == fail) & (df_all['SlowStart'] == start) & (df_all['SlowEnd'] == end)].iloc[0]\n",
    "    \n",
    "    # get machine and measurement id\n",
    "    machine, measurement = first_pump_match['MachineId'], first_pump_match['MeasurementId']\n",
    "\n",
    "    # get pressure series for given machine and cycle\n",
    "    series_pressure = df_all[(df_all['MachineId'] == machine) & (df_all['MeasurementId'] == measurement)].iloc[0, 5:].reset_index(drop=True)\n",
    "\n",
    "    # plot pressure in time for each combination\n",
    "    ax = axes.ravel()[i]\n",
    "    ax.plot(series_pressure, color='r' if fail else 'b')\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "    ax.set_title(f\"{'SlowStart' if start else ''} {'SlowEnd' if end else ''}\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# understanding data based on all possible boolean combinations\n",
    "\n",
    "# predictions based on data observation:\n",
    "# pump might fail due to fast drop at the beginning\n",
    "# slow pressure decrease at the end could also mean pump failure\n",
    "# low pressure could not necessarly mean pump failure (could be more about the pressure change speed) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# due to the data being time series, it would be probably better to create features such as:\n",
    "# how long was pump running for\n",
    "# pressure mean, variance, max\n",
    "# pressure changes in time like:\n",
    "#   - largest drop, largest spike\n",
    "#   - time series decomposition (trend, seasonality, residual) more about actuasl pressure prediction\n",
    "\n",
    "# then it could help to standardize, normalize data before doing new feature extraction\n",
    "\n",
    "df_pressure = df_all.iloc[:, 5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as I said, I want to create features as largest drop, spike, etc. so it would be best to strip actual measured sequence\n",
    "# I'll create function which will return starting and ending index based on where pressure is not zero\n",
    "# then I will drop all unnecessary columns based on the smallest starting index and largest ending index\n",
    "# this will produce same measured windows for each measured cycle but without too many zeroes\n",
    "\n",
    "ZERO_SEQ = (-1, -1)\n",
    "\n",
    "def get_nonzero_seq(df: pd.DataFrame):\n",
    "    \"\"\" Get nonzero sequence (start, end) indexes from each row of given dataframe | (-1, -1) for full empty sequences \"\"\"\n",
    "    seq = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        indexes = row[row > 0].index\n",
    "\n",
    "        if len(indexes):\n",
    "            seq.append((indexes[0], indexes[-1]))\n",
    "        else:\n",
    "            seq.append((-1, -1))\n",
    "\n",
    "    return seq\n",
    "\n",
    "seq = get_nonzero_seq(df_pressure)\n",
    "filter_seq = list(filter(lambda x: x != ZERO_SEQ, seq))\n",
    "\n",
    "seq_start, seq_end = min([x[0] for x in filter_seq]), max([x[1] for x in filter_seq])\n",
    "\n",
    "print(f\"Earliest pump activity (time): {seq_start}\")\n",
    "print(f\"Latest pump activity (time):   {seq_end}\")\n",
    "\n",
    "# largest sequence start on 34 and ends on 1065 (not necessary the same measurement cycle)\n",
    "# I'll add like 10 time ticks as a reserve to have at least some zero pressure data in the largest measurement cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary zeroes and reindex columns from zero\n",
    "reserve = 10\n",
    "\n",
    "df_pressure = df_pressure.iloc[:, seq_start - reserve:seq_end + reserve]\n",
    "df_pressure.columns = range(len(df_pressure.columns))\n",
    "df_pressure.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now I am finally going to create some new features from cycle measurements\n",
    "\n",
    "cycle_length = pd.Series([0 if idx == ZERO_SEQ else idx[1] - idx[0] for idx in seq])\n",
    "mean = df_pressure.mean(axis=1)\n",
    "var = df_pressure.var(axis=1)\n",
    "max_pressure = df_pressure.max(axis=1)\n",
    "\n",
    "print(f\"Min/Max measurement cycle length: {cycle_length.min()}, {cycle_length.max()}\")\n",
    "print(f\"Measurement cycle mean, var, std: {cycle_length.mean():.3f}, {cycle_length.var():.3f}, {cycle_length.std():3f}\")\n",
    "\n",
    "# those are the simple features\n",
    "# min measurement cycle length is 0 which is not optimal, but with given time I'll skip it for now\n",
    "# also mean of cycle length is ~205.6, so I could create new features based on longer cycle or to drop outliers based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now for the pressure change features\n",
    "# I could try to create largest pressure drop or spike in raw pressure value or in precentage\n",
    "# both should be ok, but in this case I'll use percentage\n",
    "# one thing which could be problematic is to take this change from the whole sequence, because of the inf, or 100% drop values\n",
    "# because of that I will create more slices using the reserve previously used for letting start and end zeroes  \n",
    "\n",
    "\n",
    "# now I'll create new features dataframe and then I'll concat it with the original needed features\n",
    "new_features = pd.DataFrame({\n",
    "    'Length' : cycle_length,\n",
    "    'Mean' : mean,\n",
    "    'Var' : var,\n",
    "    'MaxPressure': max_pressure,\n",
    "})\n",
    "\n",
    "new_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_new_features = pd.concat((df_all[['PumpFailed', 'SlowStart', 'SlowEnd']], new_features), axis=1)\n",
    "# df_all_new_features = df_all_new_features.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_new_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used model config\n",
    "models = {\n",
    "    'LogisticRegression' : {\n",
    "        'model' : sk.linear_model.LogisticRegression(random_state=RANDOM_STATE, max_iter=500),\n",
    "        'params' : {\n",
    "            'model__C' : np.linspace(1e-3, 1e3, 20)\n",
    "        }\n",
    "    },\n",
    "    'RandomForestClassifier' : {\n",
    "        'model' : sk.ensemble.RandomForestClassifier(),\n",
    "        'params' : {\n",
    "            'model__n_estimators' : [50, 100, 200],\n",
    "        }\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df_labels.drop(TARGET + INDEXES, axis=1), np.ravel(df_labels.loc[:, TARGET])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=RANDOM_STATE)\n",
    "\n",
    "print('Train:', X_train.shape)\n",
    "print('Test:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = {}\n",
    "\n",
    "# run gridsearchCV to tune hyperparams\n",
    "for model_name, conf in models.items():\n",
    "    model, params = conf['model'], conf['params']\n",
    "\n",
    "    pipe = Pipeline([('model', model)])\n",
    "\n",
    "    grid_search = sk.model_selection.GridSearchCV(pipe, params, scoring=make_scorer(f1_score, greater_is_better=True), cv=5)\n",
    "    grid_search.fit(X_train, y_train)   \n",
    "\n",
    "    best_models[model_name] = {\n",
    "        'model': grid_search.best_estimator_, \n",
    "        'params': grid_search.best_params_, \n",
    "        'score' : grid_search.best_score_\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get best type of model after hyperparam tuning\n",
    "model = max([(conf['score'], conf['model']) for conf in best_models.values()], key=lambda x: x[0])[1]\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "display(model)\n",
    "\n",
    "print(f\"f1 score (train):  {f1_score(y_train, model.predict(X_train)):.3f}\")\n",
    "print(f\"f1 score (test):   {f1_score(y_test, model.predict(X_test)):.3f}\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(f\"acc score (train): {accuracy_score(y_train, model.predict(X_train)):.3f}\")\n",
    "print(f\"acc score (test):  {accuracy_score(y_test, model.predict(X_test)):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df_all_new_features.drop(TARGET, axis=1), np.ravel(df_all_new_features.loc[:, TARGET])\n",
    "X_train, X_test, y_train, y_test = tt_split(X, y, test_size=0.25, random_state=RANDOM_STATE)\n",
    "\n",
    "print('Train:', X_train.shape)\n",
    "print('Test:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = sk.compose.ColumnTransformer([\n",
    "    ('OneHot', sk.preprocessing.OneHotEncoder(), ['SlowStart', 'SlowEnd']),\n",
    "    ('MinMax', sk.preprocessing.MinMaxScaler(), ['MaxPressure']),\n",
    "    ('Scaler', sk.preprocessing.StandardScaler(), ['Length', 'MaxPressure'])\n",
    "], remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = {}\n",
    "\n",
    "# run gridsearchCV to tune hyperparams\n",
    "for model_name, conf in models.items():\n",
    "    model, params = conf['model'], conf['params']\n",
    "\n",
    "    pipe = Pipeline([\n",
    "        ('prep', ct),\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "    grid_search = sk.model_selection.GridSearchCV(pipe, params, scoring=make_scorer(f1_score, greater_is_better=True), cv=5, verbose=True)\n",
    "    grid_search.fit(X_train, y_train)   \n",
    "\n",
    "    best_models[model_name] = {\n",
    "        'model': grid_search.best_estimator_, \n",
    "        'params': grid_search.best_params_, \n",
    "        'score' : grid_search.best_score_\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = max([(conf['score'], conf['model']) for conf in best_models.values()], key=lambda x: x[0])[1]\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "display(model)\n",
    "\n",
    "print(f\"f1 score (train):  {f1_score(y_train, model.predict(X_train)):.3f}\")\n",
    "print(f\"f1 score (test):   {f1_score(y_test, model.predict(X_test)):.3f}\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(f\"acc score (train): {accuracy_score(y_train, model.predict(X_train)):.3f}\")\n",
    "print(f\"acc score (test):  {accuracy_score(y_test, model.predict(X_test)):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_test, model.predict(X_test), labels=model.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['OK', 'Failure'])\n",
    "\n",
    "disp.plot()\n",
    "plt.title('Pump status confusion matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
